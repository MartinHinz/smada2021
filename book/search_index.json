[["preface.html", "Statistical methods for archaeological data analysis 1 Preface", " Statistical methods for archaeological data analysis Martin Hinz 1 Preface Hallo Welt! "],["introduction-into-r.html", "2 Introduction into R 2.1 Start R-Studio 2.2 Using R 2.3 Getting help 2.4 Assignment of data to variables 2.5 Working with variables 2.6 Using variables 2.7 Data types in R Variables 2.8 Data import through reading of files 2.9 Using c() for data entry 2.10 Named vectors 2.11 Applying functions to more complex variables 2.12 Calculations with vectors 2.13 Sequences and repeated data 2.14 Data access 2.15 Logical values 2.16 Factors 2.17 missing (NA) values 2.18 Matrices 2.19 Data frames 2.20 Data export through save", " 2 Introduction into R 2.1 Start R-Studio When we first start R Studio, we see a screen divided into several windows. On the left-hand side, directly after the start, we are greeted by the large R window, the Console. This is where the actual R programme is located. On the right, there are windows that provide further helpful functions. In the upper area we have the window in which we can see the working environment. On the one hand, there is the actual environment, marked by the tab ‘Environment’. Next to this, perhaps of interest to us at the moment, is the ‘History’ tab, in which we can see the sequence of commands entered so far. The file manager is located in the lower right-hand corner. Other tabs contain information about diagrams (plots), packages and a window in which we can use the R help system. One important window is still missing: the code or script window. This only appears when we open a new R file. To do this, either click on the plus symbol at the top left or select ‘File -&gt; New File’ from the menu. This opens another window which is placed in the top left by default and in which you enter your programme code for the analyses. This window functions as a normal text editor window, i.e. if you press Enter here, the text is not directly executed, but a new line is created. To actually execute a command, you can either click on the Run symbol in the upper area or use the keyboard shortcut Control Enter. 2.2 Using R 2.2.1 Start of the system: After R is started, you end on the prompt. &gt; This prompt expects your commands. It can be used to directly enter commands or conduct calculations like with a normal calculator. We mainly will not use R in this way.Most of the real work is done using the script window. But we can start trying out our directly using the console window. 2.2.2 Simplest way to use: R as calculator As R is an statistical program, of course it can do calculations. We can try that out by entering some basic calculations using the well-known mathematical operators. 2+2 ## [1] 4 2^2 ## [1] 4 2.2.3 Multiple commands are separated by ; If we want to enter multiple commands in one line, Either in the console or in the script window, we can separate them by using a semicolon. Each part divided by a; is treated like an individual command and is executed before the next in turn is then executed. (1 - 2) * 3; 1 - 2 * 3 ## [1] -3 ## [1] -5 2.2.4 Using functions: Beside the basic calculations R also offer us the possibility to do more complex calculations. Here we start using functions in R for the first time. Functions are commands that produce a certain output, most of the time requiring a certain input.The input usually is given by writing it in between the rounds brackets that distinguish a function call from a variable which we will see later. Functions can sometimes take more than one parameter these are then divided by, within the round brackets. In the following example in the first line of \"we calculate the square root, In the second example the natural logarithm of 10. If we would like to calculate the living room to the base of 10, we have to specify that using a second parameter. sqrt(2) #square root ## [1] 1.414214 log(10) #logarith base e ## [1] 2.302585 log(10, 10) #logarith base 10, like log(10, base=10) ## [1] 1 2.3 Getting help There is a specific function for getting help. Not surprisingly this function is called help. It takes as a parameter the name of the function for which you would like to get some information. Call of the help function: help(sqrt) Like it even simpler? You can also use the ‘?’ For getting help instead of writing the function name ‘help’. The name of the function for which you would like to have help it’s written after that ‘?’ . ? sqrt You can also search within the help files of R. Research capabilities are a limited only a fulltext search is conducted and you will not get any semantic relevant results. This means that if you would like to search for a specific topic, you probably already should know basically what you are searching for. More complicated searches probably better take place in the Internet. There are plenty of sites where you could get help or explanation how certain analyses are conducted. Searching the help: help.search(&#39;logarithm&#39;) 2.4 Assignment of data to variables A very essential concept in R is the concept of a variable. Variable can be seen as a kind of labelled drawer or replacement for an actual value that can be changed. It can become quite handy if for example you are writing a script or analyses, In which certain values might be changed in individual runs. Here you can define a replacement for the actual value that is the variable and specify the content of the variable for example in the beginning of the analyses. Here it can easily be changed if necessary. Setting the value of a variable is also called assignment. If we assign a value to a variable are is not reporting any message back. If we want to see the content of the variable we have to enter this variable itself without any other additions. There are some data shipped with our. We will talk about datasets later. Some inbuilt constants are the letters of the alphabet, the names of the month and also the value of pi. x &lt;- 2 # no message will be given back x ## [1] 2 There are some data shipped with our. We will talk about datasets later. Some inbuilt constants are the letters of the alphabet, the names of the month and also the value of pi. pi # build in variable ## [1] 3.141593 When selecting variable names you’re quite free to choose. It is necessary, that the name of the variable starts with the letter. You should avoid using mathematical signs, because they could be interpreted as actual calculation. This means, you should not use the minus sign, but you’re perfectly free to use the underscore \"_\" or the dot “.”. 2.4.1 Arrow or equal sign? There are different options for the assignment sign in our. The traditional one is the arrow composed of a ‘smaller than’ sign and minus sign. Most other programming languages and now also our takes the = as an assignment. What you would like to use as a matter of taste. Personally I’d like the Aero more because it is more speaking and more clear. Classic assignment symbol in R is the arrow. Also possible: x=2 Both are possible. 2.5 Working with variables And this is helpful to get an overview about which variables we have already Defined. For this in our studio in the right hand area there is the environment window. If we want to get an overview about the assigned variables in our itself, we can use the command ls(). Currently there is only one variable in our environment. That is the variable X that we just assigned. Display of already uses variables: ls() ## [1] &quot;x&quot; Sometimes it might be helpful to get rid of one of the variables. To do this you can use the rm() command. This stands for remove. The name of the variable that has to be deleted is given within the round brackets ending the function call. If we after the removal of a variable get a listing of the variable environment again the variable should have gone. Delete a variable: rm(x) # no message will be given back ls() ## character(0) 2.6 Using variables Already have been said a variable can be used instead of an actual value. To do this we simply replace the use of the value with the name of the variable. For example if we want to use a variable when we calculate 2×2 we can at first assign 2 to one Variable and use it instead of actually writing to in our calculation. An important concept is also that the result of the calculation can also be assigned to a variable. With this we can chain analyses together and use the output of one of the functions as the input of the next function. In our example we assign to to the variable x, then we double its value and assign the results to the variable y. The result of this calculation is then used to calculate the square roots using the function sqrt(). Calculations with variables: x &lt;- 2 y &lt;- 2 * x z &lt;- sqrt(y) # no message will be given back No using the function ls(), We can’t get an overview over our current environment. We should see now the 3 variable that we have created. Additionally if we inspect the individual variables, we shall see that y contains the value of four while z contains the value of two. ls() ## [1] &quot;x&quot; &quot;y&quot; &quot;z&quot; y ## [1] 4 z ## [1] 2 Exercise 2.1 (Calculation of a circle) Given is a circle with the radius r=5. Calculate the diameter d (2 * r), the circumference u (2 * π * r) and the area a (π * r^2). Add area a and circumference u, assign the result to the variable v and delete u and a. Solution r &lt;- 5 d &lt;- 2 * r u &lt;- 2 * pi * r a &lt;- pi * r^2 v &lt;- a + u rm(u) rm(a) 2.7 Data types in R Variables There are four main data types in R: Scalars, vectors, matrices, data frames. 2.7.1 Scalar Scalar are individual values. This can be numbers text strings or true/false values. The essential characteristic is that it is only one value that is represented by a scalar. Examples of Scalar are all those variables that we used until now. pi ## [1] 3.141593 All these variables stored only one value at the time. 2.7.2 Vector A vector is a variable that holds multiple values at the time in a one-dimensional data structure. You can’t imagine it as a kind of list where every item off the list again is a scalar. We have already seen an example of a vector: the result of the listing of the variables, resulting from the command ls() represents a vector, where every position in this vector holds a scalar information, that is the name of the variable. ls() ## [1] &quot;d&quot; &quot;r&quot; &quot;v&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; 2.7.3 Matrix: A vector is a one-dimensional data structure. If we add more dimensions to this idea, we end up with a Matrix. In the simplest implementation you can imagine a matrix as a table with rows and columns. That we have rows and columns represents the two-dimensionality of this data structure. Matrices with more dimensions are easily implementable, although our imagination probably will stop with three dimensions. Most of the time we will use two-dimensional matrices. As with vectors, each element in a matrix represents a scalar value. One of the specific features of the data type matrix in R is, that all values have to be of the same kind. That means with in one and the same metrics, there can only be numbers, characters, or true and false values at once. We can’t mix these types of information in a matrix, which is the difference from the next data structure that we will learn. There are also inbuilt matrices in R, for example are matrix holding the transfer rates between different European currencies. Of course these are restoring values and not updated online all the time euro.cross ## ATS BEF DEM ESP FIM FRF IEP ITL LUF NLG PTE ## ATS 1.000000000 2.93161486 0.142135709 12.0917422 0.432093050 0.476702543 0.0572345080 140.714229 2.93161486 0.160149851 14.5695951 ## BEF 0.341108927 1.00000000 0.048483759 4.1246012 0.147390797 0.162607493 0.0195232016 47.998880 1.00000000 0.054628544 4.9698190 ## DEM 7.035529673 20.62546336 1.000000000 85.0718109 3.040003477 3.353854885 0.4026750791 989.999131 20.62546336 1.126739032 102.5048189 ## ESP 0.082701069 0.24244768 0.011754775 1.0000000 0.035734557 0.039423810 0.0047333550 11.637217 0.24244768 0.013244564 1.2049211 ## FIM 2.314316324 6.78468413 0.328946992 27.9841163 1.000000000 1.103240477 0.1324587561 325.657236 6.78468413 0.370637415 33.7186519 ## FRF 2.097744212 6.14977811 0.298164361 25.3653822 0.906420695 1.000000000 0.1200633578 295.182459 6.14977811 0.335953424 30.5632839 ## IEP 17.471976881 51.22110711 2.483391826 211.2666399 7.549519785 8.328935807 1.0000000000 2458.555749 51.22110711 2.798134501 254.5596294 ## ITL 0.007106602 0.02083382 0.001010102 0.0859312 0.003070713 0.003387735 0.0004067429 1.000000 0.02083382 0.001138121 0.1035403 ## LUF 0.341108927 1.00000000 0.048483759 4.1246012 0.147390797 0.162607493 0.0195232016 47.998880 1.00000000 0.054628544 4.9698190 ## NLG 6.244151907 18.30544854 0.887516960 75.5026750 2.698054644 2.976603092 0.3573809621 878.641019 18.30544854 1.000000000 90.9747653 ## PTE 0.068636087 0.20121457 0.009755639 0.8299299 0.029657176 0.032718997 0.0039283527 9.658074 0.20121457 0.010992059 1.0000000 You can see that we have rows and columns here and both the rows and columns have names. In this example row and column names are the same because we have a special kind of matrix. But in general the names of the rows and columns can differ from each other. Matrices are specific data types with which you can conduct matrix algebra, which is a specific branch of mathematics that is also used in statistics. We will not deal with this very much. That’s why we most of the time will probably work more with the next data type. 2.7.4 Data frame: The fourth of our data types is the data type data.frame. Similar to the matrix, this datatype represents a more than one dimensional data storage unit. Different from the matrix, in data frames values of different kinds can be stored. More specifically the different columns of the data frame can differ in respect of the contains data type. That means we can combine columns that have character values with columns that hold numeric values. Tables in data frames are usually structured in a specific way: the rules usually hold the item off on investigation or the observations, while the columns usually holds the different features or variables of interest. One example of such a data frame that is inbuilt in our is the data frame mtcars. This data frame contains the technical details of different cars. Also this is more a historical dataset. mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 You can see, that the real names contains the names of the different cars, identifying them. The column names contains different measurements or information is, there are specific for the individual cars. The individual values identified by row and column then holds to specific values that are unique for this individual item or car. Data frames are the data type that we will use most of the time, especially if we import data from other sources. How we can do that, will be shown subsequently. But at first we have to make sure, that we get our data from the right location on our computer. For that we need to concept of the working directory. 2.7.5 The working directory Historically, R is a software that has always been run within the console. Therefore it expects all its inputs from a specific folder on your computer, the working directory. Also, if any output is written to the disk on your computer, this also will take place in the specified working directory. Of course this working directory is not fixed, but you can specify that according to your specific workflow. At first we can use the command getwd() to see where on the computer I will working direct with currently is located. getwd() Then we can use the command setwd(\"your/working/directory\") to set this working directory to a specific folder of your computer. setwd(&quot;U:\\R&quot;) # or something else How specific folder has to be addressed, depends on the operating system. While Linux and macOS computers treat directory name is more or less the same, in Windows computers the path is prepared by the volume letter. With RStudio, there are different other options how are you can use the graphical user interface to specify the working directory. This might be more convenient than typing the path, especially if you are not used to it. You will find options for this in the files window of our studio, under the icon ‘More’, or in the main menu under the item ‘session’. Change the path according to your needs. Also you can make it a habit to check in the beginning of every R session, what do you work in directory is and if it is correctly specified. 2.7.6 Download data for further tasks into your working directory In the reminder of the chapter we will need some files that can be downloaded using the following links: height.RData kursmatrix.txt kursdata.txt kursdata.csv Please save these files to the directory that you have defined as your working directory. In the following the used example will assume that the files are accessible directly, as they should be if they are placed in the working directory. Remember: getwd() setwd(&quot;my/location/of/my/working/directory&quot;) 2.8 Data import through reading of files Data can be imported into R from different formats and sources. The most straightforward version is to directly scan a text file and read it into an R variable. For directly reading in a file we can use the function scan(). The file kursmatrix.txt is a simple text file in which ages and bodies sizes of individuals are listed consecutively. Scan reads in each item and translate it to a position in a vector. scan(&quot;kursmatrix.txt&quot;) ## [1] 39 34 23 38 23 21 23 31 25 31 24 23 23 39 21 181 170 185 163 175 163 162 172 172 180 187 158 184 156 168 If we, for example, want to turn this factor into a two-dimensional structure, like a matrix, we can use the command matrix to define such a structure and then use as an input the scanned content of the file. For the command matrix(), One of its parameters is the content that should be turned into a matrix, the second parameter is the number of columns that this matrix should have in end. kursmatrix &lt;- matrix(scan(&quot;kursmatrix.txt&quot;),ncol=2) The result is a two-dimensional structure, with two columns, in which body height and age are listed in different columns. kursmatrix ## [,1] [,2] ## [1,] 39 181 ## [2,] 34 170 ## [3,] 23 185 ## [4,] 38 163 ## [5,] 23 175 ## [6,] 21 163 ## [7,] 23 162 ## [8,] 31 172 ## [9,] 25 172 ## [10,] 31 180 ## [11,] 24 187 ## [12,] 23 158 ## [13,] 23 184 ## [14,] 39 156 ## [15,] 21 168 The file kursdata.txt contains a more complicated data structure. Here we have information is of different kinds, for example strings, but also numeric values. This kind of data can be imported into an data frame. The most general function to import table data is the function read.table(). kursdata &lt;- read.table(&quot;kursdata.txt&quot;) One of the most widely used text file for exchange of numerical and other data are those in the CSV format. This format comes into flavours, Differentiated by the character that separates the columns. The original CSV format has a column separator “,” and a decimal separator using “.”. In European and other countries the “,” it’s often used as decimal separator. Therefore also a CSV2 format exists. Here the column separator is a “;”, while the decimal separator is, \". In Switzerland most of the time we will probably use the CSV2 format. In this format we have the same data available like we have in the kursdata.txt, the file is now called kursdata.csv. kursdata &lt;- read.csv2(&quot;kursdata.csv&quot;) kursdata ## X age height sex ## 1 Matthias 39 181 m ## 2 Jannick 34 170 m ## 3 Nicolas 23 185 m ## 4 Silvia 38 163 f ## 5 Till 23 175 m ## 6 Anna 21 163 f ## 7 Ilaria 23 162 f ## 8 Sarah 31 172 f ## 9 Clara 25 172 f ## 10 Alain 31 180 m ## 11 Adrian 24 187 m ## 12 Marlen 23 158 f ## 13 Michael 23 184 m ## 14 Helena 39 156 f ## 15 Nephele 21 168 f If we read in the data like this, you will realise, that there is a numeric naming, that is automatically given by R. If the dataset already consists of a unique identifier, that is a value, that is not repeated within the whole dataset, and that uniquely identify every individual item of the dataset, this can be used instead of the numeric identifier. This unifier of individual items is called row names in R. So if we specify in the read.CSV2 command, that we want to use for example the first column as row names, we can do it like this. kursdaten &lt;- read.csv2(&quot;kursdata.csv&quot;,row.names = 1) kursdaten ## age height sex ## Matthias 39 181 m ## Jannick 34 170 m ## Nicolas 23 185 m ## Silvia 38 163 f ## Till 23 175 m ## Anna 21 163 f ## Ilaria 23 162 f ## Sarah 31 172 f ## Clara 25 172 f ## Alain 31 180 m ## Adrian 24 187 m ## Marlen 23 158 f ## Michael 23 184 m ## Helena 39 156 f ## Nephele 21 168 f 2.9 Using c() for data entry Now we know how we can assign more complicated data sets two variables by loading them from the file system. Sometimes, it might also be necessary, to directly assign more than one value to a variable. Let’s start with the example of a vector. A vector is created in R using the command c(). This ‘c’ stands for combine, and enables us to combine multiple values to be assigned to a variable, but also for different purposes. Let’s assume that we would like to make a vector of different Bronze Age sites. We assign the result to a variable called places. places &lt;- c(&quot;Leubingen&quot;, &quot;Melz&quot;, &quot;Bruszczewo&quot;) As in every other situation, in R actual value can be replaced with a variable. Also when we combine values we can not only combine actual values, in this case strings, but we also could use variables and combined them with other variables. To demonstrate that let’s make another vector of side categories that we call categories. categories &lt;- c(&quot;burial&quot;, &quot;depot&quot;, &quot;settlement&quot;) categories ## [1] &quot;burial&quot; &quot;depot&quot; &quot;settlement&quot; Now we can combine these two factors into one. c(places, categories) ## [1] &quot;Leubingen&quot; &quot;Melz&quot; &quot;Bruszczewo&quot; &quot;burial&quot; &quot;depot&quot; &quot;settlement&quot; 2.10 Named vectors We already learnt to concept of row names and column names. Also places in a vector can have a specific identifier, the name. Since vectors do not have rows and columns, this feature is called only called ‘name’. We can use another vector to assign names, or we could directly enter names for the individual positions. In this case we use our category vector as base vector and the sites in the places vector as identifiers. names(categories) &lt;- places categories ## Leubingen Melz Bruszczewo ## &quot;burial&quot; &quot;depot&quot; &quot;settlement&quot; The result is a vector, in which every position has the name of the site is unique identifier, and where the values are the site categories for this specific archaeological sites. 2.11 Applying functions to more complex variables Also variables with more complex content can, of course, be used in calculations and other functions. Due to their nature, and the fact that they contain more than one value, this of course changes the range of functions that can be applied to them. I will demonstrate that with a reduced version of our data. We will use only a vector of the body height of the individuals. For this we explore a way of loading data into R. This time we use the need to data storage option of R. This format is called ‘RData’, and different from other loading or saving, we do not have to specify a variable name. In this case the variable is stored with its content, and if we load this dataset again, the variable is restored with the same name. load(&quot;height.RData&quot;) height ## Matthias Jannick Nicolas Silvia Till Anna Ilaria Sarah Clara Alain Adrian Marlen Michael Helena Nephele ## 181 170 185 163 175 163 162 172 172 180 187 158 184 156 168 Now we can use this vector that is assigned to the variable name height, to demonstrate some functions that make calculations over all the values that are stored in this vector. The first step probably comes to mind, is to sum up all the values. This can be done in R own using the function sum(). # Sum: sum(height) ## [1] 2576 We can also count the number of values in the vector. The command for this is length(). # Count: length(height) ## [1] 15 If we have the number of cases, and to some of their individual values, we easily can calculate the arithmetic mean. # Mean: sum(height)/length(height) ## [1] 171.7333 Since this is a very essential statistical value or parameter, of course there exists a specific command for this in R. There is no big surprise that this function is called mean(). # Or more convenient: mean(height) ## [1] 171.7333 Other possible functions might for example be related to the order and the extremes of the values within our dataset. We can sort the dataset according to the values, using the function sort(). In case of numerical values, the items will be sorted according to the numerical order. In case of characters, the items will be sorted according to the character. Our height data on numerical, therefore we will get them sorted from the smallest to the largest person. # sort: sort(height) ## Helena Marlen Ilaria Silvia Anna Nephele Jannick Sarah Clara Till Alain Matthias Michael Nicolas Adrian ## 156 158 162 163 163 168 170 172 172 175 180 181 184 185 187 Immediately we can identify the smallest and the largest person. But we can also explicitly get the values using the function min() for minimum, and max() for maximum. The function range() gives both values at the same time. # minimum: min(height) ## [1] 156 # maximum: max(height) ## [1] 187 # Or both at the same time: range(height) ## [1] 156 187 2.12 Calculations with vectors Not only can we use functions on more complex variables like vectors, we also can do calculations. If, for example, we combine a scalar value With a mathematical expression with a vector, the calculation is done at every position of this vector. For example, if we want our height vector in metre, we have to divided by 100. We can directly apply this calculation to the whole variable, and the results will change every individual position in that vector. That means, we divide the variable by 100, and all the items in the variable are then divided by 100, causing every value to be in meter instead of centimeter. height.in.m &lt;- height/100 height.in.m ## Matthias Jannick Nicolas Silvia Till Anna Ilaria Sarah Clara Alain Adrian Marlen Michael Helena Nephele ## 1.81 1.70 1.85 1.63 1.75 1.63 1.62 1.72 1.72 1.80 1.87 1.58 1.84 1.56 1.68 The case is different if we combine to vectors with a mathematical expression. In this case, the first value of the first vector is combined with the first value of the second vector.The second value of the first vector is then combined with the second value of the second vector, and so forth. test&lt;-c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15) height.in.m + test ## Matthias Jannick Nicolas Silvia Till Anna Ilaria Sarah Clara Alain Adrian Marlen Michael Helena Nephele ## 2.81 3.70 4.85 5.63 6.75 7.63 8.62 9.72 10.72 11.80 12.87 13.58 14.84 15.56 16.68 In case, that we have different number of positions in the individual factors vectors, the short one is “recycled”. That means, it starts again from the beginning. You can try that out yourself, if you take the example above and remove some items from the test vector. Exercise 2.2 (Data collection lithics) An excavation produced the following numbers of flint artefacts: flakes blades cores debris 506 104 30 267 Assign the values to a named vector, calculate the proportion of the artefacts and sort the vector according to their percentage During the data collection on box with artefacts was missing, the following numbers has to be added to the vector: flakes blades cores debris 52 24 15 83 Moreover were 10 items each artefact type missing. Make a vector for the box, add it and the 10 missing to the original data and repeat the calculations. Solution artefacts &lt;- c(506, 104, 30, 267) names(artefacts) &lt;- c(&quot;flakes&quot;, &quot;blades&quot;, &quot;cores&quot;, &quot;debris&quot;) prop &lt;- artefacts/sum(artefacts) sort(prop) ## cores blades debris flakes ## 0.03307607 0.11466373 0.29437707 0.55788313 missing_box &lt;- c(52,24,15,83) all_artefacts &lt;- artefacts + missing_box + 10 prop &lt;- all_artefacts/sum(all_artefacts) sort(prop) ## cores blades debris flakes ## 0.04906334 0.12310437 0.32114184 0.50669045 Variant: We also could have over written the content of the artefact variable with the new values including the missing box and the 10 additional items. In that case the court would look like this: artefacts &lt;- artefacts + missing_box + 10 prop &lt;- artefacts/sum(artefacts) sort(prop) ## cores blades debris flakes ## 0.04906334 0.12310437 0.32114184 0.50669045 You see, that artefact is twice present in the first line. This is possible, because the right-hand side of the assignment is evaluated first, and then the result is assigned to the actual variable. This technique can also be used in actual scripts if you don’t need the intermediate values of the variable. It can become quite handy, to reduce the amount of variables and doing names. But you always will have to take care: you lose the intermediate values! So if you have to repeat any step in between, or later you would need some of the intermediate values you will not have them. 2.13 Sequences and repeated data Now we have seen, how we can produce vectors ourselves, and how we can use them in calculations. There are some specific vectors, either consisting of the repetition of an individual value, or sequences of values. There are some inbuilt functions in R that can help you producing these kinds of vectors fast. Let’s start with a simple sequence. Let’s assume, that we need the values from 1 to 10. We can produce such a simple sequence rather easily like this: 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 But also more complicated sequences are possible. For this we need an explicit function call for the function seq(). This command takes several parametres, the first one is the starting value, the second one the end value. You can also define the increment using the parameter by, or the desired length of the resulting vector, using the power meter length. seq(1,10,by=2) ## [1] 1 3 5 7 9 seq(1,20,length=5) ## [1] 1.00 5.75 10.50 15.25 20.00 You can check out other options and use cases indeed help documentation for this command. The other mentioned option, the repetition, works for letters as well as for numeric values. The command here is rep(). Here, the first parameter is the value that should be repeated. This value can also be a vector. The second para meter is the number of times, that this value should be repeated. Also hear further options can be found in the documentation of the command. rep(1,10) ## [1] 1 1 1 1 1 1 1 1 1 1 rep(1:3,3) ## [1] 1 2 3 1 2 3 1 2 3 rep(c(&quot;Anton&quot;,&quot;Berta&quot;,&quot;Claudius&quot;),3) ## [1] &quot;Anton&quot; &quot;Berta&quot; &quot;Claudius&quot; &quot;Anton&quot; &quot;Berta&quot; &quot;Claudius&quot; &quot;Anton&quot; &quot;Berta&quot; &quot;Claudius&quot; 2.14 Data access 2.14.1 by index/position And important possibility is to access data with in such a complex structure like for example a vector. By convention, for accessing data in R, square brackets are used. Indicates of a one-dimensional data structure, within the brackets you can give the position of the item that you would like to access. This can be an individual number, a vector of numbers, Or, by using the minus sign, you can also exclude eighter individual value or a range of values. Here, sequences can become very handy. height[1] ## Matthias ## 181 height[5] ## Till ## 175 height[1:3] ## Matthias Jannick Nicolas ## 181 170 185 height[-(1:3)] ## Silvia Till Anna Ilaria Sarah Clara Alain Adrian Marlen Michael Helena Nephele ## 163 175 163 162 172 172 180 187 158 184 156 168 If we have a named vector, like for example with our heigth data, these positions have also and unique identifier. In that case, we can also use the unique identifier, to access a specific position in our data storage vector. height[&quot;Clara&quot;] ## Clara ## 172 This data access is two ways: not only can we get the values at a specific position, but we can also change the values, given that we indicate a specific position in the vector. In the following example at first the content of the vector height is shown, then we change the entry in the first value, and you can inspect the effect. height ## Matthias Jannick Nicolas Silvia Till Anna Ilaria Sarah Clara Alain Adrian Marlen Michael Helena Nephele ## 181 170 185 163 175 163 162 172 172 180 187 158 184 156 168 height[1] &lt;- 168 height ## Matthias Jannick Nicolas Silvia Till Anna Ilaria Sarah Clara Alain Adrian Marlen Michael Helena Nephele ## 168 170 185 163 175 163 162 172 172 180 187 158 184 156 168 Of course the same is true for the access by name. height[&quot;Till&quot;] &lt;- 181 height ## Matthias Jannick Nicolas Silvia Till Anna Ilaria Sarah Clara Alain Adrian Marlen Michael Helena Nephele ## 168 170 185 163 181 163 162 172 172 180 187 158 184 156 168 2.15 Logical values Until now we had only vectors or other variables that stored either numeric values or strings. No we learn another category of data type: the logical values. These are also called binary, boolean, or true/false values. These values can result from inequations or checks: pi&gt;4 ## [1] FALSE height &gt; 175 ## Matthias Jannick Nicolas Silvia Till Anna Ilaria Sarah Clara Alain Adrian Marlen Michael Helena Nephele ## FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE but you can also enter them yourself. Logical values are entered as ‘TRUE’ or ‘FALSE’. But there is also a shortcut, ‘T’ or ‘F’ would be enough. logic_test &lt;- c(T,F) logic_test == T ## [1] TRUE FALSE logic_test == F ## [1] FALSE TRUE Above you can also see another specific way of how an equation sign is used in our in a comparison. In this situation, two ‘=’ are used to distinguish it from the assignment situation. Comparisons, and the resulting logical values, can become very helpful when selecting specific values in a dataset. For example, if you want to select all the individuals that are larger than 1 m 75, you can do that by including a comparison in the square brackets used for accessing data. You can also use the command which() to identify in which cases a certain comparison would be true. Lastly, logical values are internally sorted as 0 and 1, and can therefore also be used in calculations or counts. For example, if we want to identify, how many percent of our individuals are larger than 1 m 75, we can sum the results from this comparison. In case that this comparison would return true, it would also return one. By summing up the ones, we get a count. Dividing the count by the number of cases, we get the percentage. height[height&gt;175] ## Nicolas Till Alain Adrian Michael ## 185 181 180 187 184 which(height&gt;175) ## Nicolas Till Alain Adrian Michael ## 3 5 10 11 13 sum(height&gt;175)/length(height) ## [1] 0.3333333 2.16 Factors The last type of information are factors. A factor is a codified textual information that is within a very specific range of values. An example for a factor might be the sex of an individual. From the biological determination, this can result in male, female, or undetermined. This means we have only three values. The difference between a factor variable and character variable is, that internally the values are stored as numbers. The table translates then the number to the actual textual representation. sex &lt;- factor(c(&quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;)) sex ## [1] m m m f m f f f f m m f m f f ## Levels: f m Another specific feature of factor variables is that they can also represent ordered values. We might see this later. 2.17 missing (NA) values Missing values are annoying in every kind of investigation. They have to be treated in a specific way, distinguishing them from the situation where the value is zero. If we have a value that is zero, this means we have information that the value is actually zero. In our example you can see the effect. If we set the height of an individual person to 0, and then calculate the mean, we get the wrong result. height[&quot;Marlen&quot;] &lt;- 0 mean(height) ## [1] 160.7333 sum(height)/14 ## [1] 172.2143 So this can cause problems, if we would use the 0 as an encoding for missing information. For this purpose there is a specific value called ‘not available’ or NA. If we set the value of an individual item to not available NA, and then calculate the mean, the result is NA. This is a warning sign, that in the dataset there are missing cases. We can use the parameter na.rm=T, read NA remove it’s true, to ignore all the NAs and to conduct the calculation of the mean value. This is true for a lot of other functions. height[&quot;Marlen&quot;] &lt;- NA mean(height) ## [1] NA mean(height, na.rm=T) ## [1] 172.2143 2.18 Matrices We initially have already talked about the matrices, Two or more dimentional data storage, which also can be used in mathematical procedures. This of course is only true, if the matrix contains numerical values only. And, as we have already seen, do matrices also have names. Since we talk about more dimensional objects, we have to be specific, about which names we talk. That is because in the case of matrices, but also in the case of data frames, we talk about row names and column names. We already have loaded the information about people in the form of the kursmatrix. kursmatrix ## [,1] [,2] ## [1,] 39 181 ## [2,] 34 170 ## [3,] 23 185 ## [4,] 38 163 ## [5,] 23 175 ## [6,] 21 163 ## [7,] 23 162 ## [8,] 31 172 ## [9,] 25 172 ## [10,] 31 180 ## [11,] 24 187 ## [12,] 23 158 ## [13,] 23 184 ## [14,] 39 156 ## [15,] 21 168 This is already in the conventional representation: the rows contain information about a specific item, the columns contain each specific variable. To make this more clear, we should assign row and column names. Also here, like with the names for vectors, we can use either variables or actual values. rownames(kursmatrix) &lt;- names(height) colnames(kursmatrix)&lt;-c(&quot;age&quot;, &quot;height&quot;) kursmatrix ## age height ## Matthias 39 181 ## Jannick 34 170 ## Nicolas 23 185 ## Silvia 38 163 ## Till 23 175 ## Anna 21 163 ## Ilaria 23 162 ## Sarah 31 172 ## Clara 25 172 ## Alain 31 180 ## Adrian 24 187 ## Marlen 23 158 ## Michael 23 184 ## Helena 39 156 ## Nephele 21 168 Like with vectors, mathematical operations are possible with matrices. Actually that is a their prime purpose. For example, we can divide a metrics by 100 or any other scalar value. The result will be a matrix, in which every individual value is divided by this scalar, in the specific case 100. kursmatrix / 100 ## age height ## Matthias 0.39 1.81 ## Jannick 0.34 1.70 ## Nicolas 0.23 1.85 ## Silvia 0.38 1.63 ## Till 0.23 1.75 ## Anna 0.21 1.63 ## Ilaria 0.23 1.62 ## Sarah 0.31 1.72 ## Clara 0.25 1.72 ## Alain 0.31 1.80 ## Adrian 0.24 1.87 ## Marlen 0.23 1.58 ## Michael 0.23 1.84 ## Helena 0.39 1.56 ## Nephele 0.21 1.68 We can also access individual values within a matrix. This is done in the same way like with vectors. So either, using the position in the form of a number, or by name. Since now we have a more dimensional data object, we also have more dimensions to specify, if we would like to access a specific value. In the case of a two-dimensional matrix, for example, we have to give two positions to identify a specific value. These positions are separated by a comma. General, rows are the first dimension, while columns are the second dimension in our. So rows first, Callums second is a rule, that is applicable for a lot of other situations. If we specify only one of the positions, we refer to either the whole column, or the whole row. The result is then again a vector. Also on this selection, like on every other vector, we can apply mathematical operations. kursmatrix[, 1] / 100 ## Matthias Jannick Nicolas Silvia Till Anna Ilaria Sarah Clara Alain Adrian Marlen Michael Helena Nephele ## 0.39 0.34 0.23 0.38 0.23 0.21 0.23 0.31 0.25 0.31 0.24 0.23 0.23 0.39 0.21 Also in this case, if we combine a matrix with a vector, the same logic is to like if we combine to vectors. So if we combine a mattress and a vector, Every value of the vector is combined with every value of the Matrixx starting with the first vector within the matrix. If we combine a matrix and the matrix, then the first value in the first column of the first matrix is combined with the first value of the first column in the second matrix, and so on, equivalent to the way in which vectors are combined. kursmatrix / c(1:15, rep(2, 15)) ## age height ## Matthias 39.000000 90.5 ## Jannick 17.000000 85.0 ## Nicolas 7.666667 92.5 ## Silvia 9.500000 81.5 ## Till 4.600000 87.5 ## Anna 3.500000 81.5 ## Ilaria 3.285714 81.0 ## Sarah 3.875000 86.0 ## Clara 2.777778 86.0 ## Alain 3.100000 90.0 ## Adrian 2.181818 93.5 ## Marlen 1.916667 79.0 ## Michael 1.769231 92.0 ## Helena 2.785714 78.0 ## Nephele 1.400000 84.0 To get a feeling for these rules, it is best that you try out different combinations, and observe the results. 2.19 Data frames The last of the major data types, that we have already seen, is the data frame. A data frame results either from the import of a CSV file, or it can be created on the spot in R by combining different vectors in a more dimensional table. These factors also can come from a matrix. For this we used to command data.frame(), which constructs a data frame. The columns are their names are given in this construction, and their values are assigned with an = after this. You aware, that we do not assign actually in this example a variable age with the values of the matrix, but only a column within the data frame. That is one of the reasons, why the syntax using the assignment arrow is more clear, because it differentiate from this construction of the data frame. kursdata &lt;- data.frame(age = kursmatrix[,1], height = kursmatrix[,2], sex=sex) kursdata ## age height sex ## Matthias 39 181 m ## Jannick 34 170 m ## Nicolas 23 185 m ## Silvia 38 163 f ## Till 23 175 m ## Anna 21 163 f ## Ilaria 23 162 f ## Sarah 31 172 f ## Clara 25 172 f ## Alain 31 180 m ## Adrian 24 187 m ## Marlen 23 158 f ## Michael 23 184 m ## Helena 39 156 f ## Nephele 21 168 f Also in the case of a data frame, very similar to the situation with the matrix, we can access individuals rows or columns by either index or name. In case of the data frame there is specific notation for accessing the content of a specific column: for this we can use the $. kursdata[,&quot;age&quot;] ## [1] 39 34 23 38 23 21 23 31 25 31 24 23 23 39 21 kursdata$age ## [1] 39 34 23 38 23 21 23 31 25 31 24 23 23 39 21 Like with matrices, we can use data frames in calculations. Since in a DataFrame also non-numerical values can be stored, this is not always make sense. But we can use the notation above to specify individual columns and assign calculations to them. Additionally a very useful command can be the command summary(). This gives you a summary of the individual columns of data frame, but can also be used with other objects in R. The way in which the summary is conducted might depend on the specific object. kursdata$height / 100 ## [1] 1.81 1.70 1.85 1.63 1.75 1.63 1.62 1.72 1.72 1.80 1.87 1.58 1.84 1.56 1.68 summary(kursdata) ## age height sex ## Min. :21.00 Min. :156.0 f:8 ## 1st Qu.:23.00 1st Qu.:163.0 m:7 ## Median :24.00 Median :172.0 ## Mean :27.87 Mean :171.7 ## 3rd Qu.:32.50 3rd Qu.:180.5 ## Max. :39.00 Max. :187.0 tapply(kursdata$height, kursdata$sex, mean, na.rm=T) ## f m ## 164.2500 180.2857 The last line in this piece of code above is an example, how are you can use this $notation very handy in function calls. This example applies to the vector height in the dataset the calculation of the mean, differentiated by the sex, and also ignores potentially NA values. This kind of notation is very close to what you probably will use later on a lot in your actual analyses. There are several datasets inbuilt in R that can be used for experimentation or testing out certain functionalities. You can get a list of this using the command data(). The resulting list might be very long, its length depends on the number of packages that you have installed. The list below only serves as an example. You have to try it out yourself, if you want to have the full list. data() Data sets in package &#39;datasets&#39;: AirPassengers Monthly Airline Passenger Numbers 1949-1960 BJsales Sales Data with Leading Indicator BJsales.lead (BJsales) Sales Data with Leading Indicator BOD Biochemical Oxygen Demand CO2 Carbon Dioxide Uptake in Grass Plants ChickWeight Weight versus age of chicks on different diets DNase Elisa assay of DNase EuStockMarkets Daily Closing Prices of Major European Stock Indices, 1991-1998 Formaldehyde Determination of Formaldehyde HairEyeColor Hair and Eye Color of Statistics Students Harman23.cor Harman Example 2.3 2.20 Data export through save Finally, if we finished our analyses, most of the time we also need to export some data. This is an analogy to the options to read data into our. The most basic option would be to directly write a simple text file. With this option you lose a lot of the internal structure of the dataset, and it is not for certain, that it will be imported in the right way. write(kursmatrix,&quot;kursmatrix.txt&quot;) Specially, if you have a data frame, it makes more sense to write it directly as a table. With the command wright.table() you can specify a lot of options how the dataset will be stored. If you’d like to know, please consult do you documentation. write.table(kursdata,&quot;kursdata.txt&quot;) But most of the time, you will probably not need all the flexibility, that ride table gives you. Most of the time, you will like to write a CSV file, because this is the standard exchange file between R and a lot of other software, including spreadsheet software like Microsoft Excel. write.csv2(kursdata,&quot;kursdata.csv&quot;) As we have said earlier, please pay attention to the language setting off your computer. Most of the time, at least in Switzerland, you will have a European continental setting, where the decimal separator is a comma. In that case, it is likely that you would like to use the CSV2 format. To try out the differences, you can run the cockpit below, and open the resulting file in your spreadsheet software. You can also inspect it with a text editor. kursdata$height &lt;- kursdata$height/100 write.csv(kursdata,&quot;kursdata.csv&quot;) You very likely will have problems with Microsoft Excel. Over spreadsheet software might be smarter. Nevertheless, if you are on the continent, you would like to save your data in the way like below. write.csv2(kursdata,&quot;kursdata.csv&quot;) Of course, R also offer us packages, that directly can save files in .XLSX format. The downside of these packages are, that most of the time they require additional dependencies or programming languages, for example perl or python. And actually using the CSV format this is not necessary at all. So it is best that you develop the habit to use CSV as you exchange format between different software. With this, now I hope, that we have everything at hand, so that we can start using R as actual software. In the next chapter we will start producing that part of statistics, that most people think of when statistics are mentioned: graphs, diagrams, and tables. "],["explorative-statistics-graphical-display.html", "3 Explorative statistics &amp; graphical display 3.1 Dataset used for this chapter 3.2 Cross tables (contingency tables) 3.3 Basics about charts 3.4 The command plot() 3.5 Export the graphics 3.6 Pie chart 3.7 Bar plot 3.8 Box-plot (Box-and-Whiskers-Plot) 3.9 Scatterplot 3.10 Histogramm 3.11 stem-and-leaf chart 3.12 kernel smoothing (kernel density estimation) 3.13 Guidelines", " 3 Explorative statistics &amp; graphical display (The videos for this chapter are here, Number 10-16) Like I already described in the introduction, statistics can be divided into different subfields. On the one hand there is the statistical inference, dealing with the testing of hypothesis on data. On the other hand there are fields like explorative statistics, in which the detection of pattern in the data is in the foreground. Decides that there also exists the field of descriptive statistics, in which parameters or distributions of data are the main object of investigation. In this chapter we will deal with explorative statistics and descriptive statistics kind of at the same time, because we will learn how to visualise data with the help of R. Especially graphical visualisation is in between descriptive and explorative statistics. 3.1 Dataset used for this chapter The dataset, that we are using for this chapter, comes from the burial-ground of Münsingen/Rain. Maybe in the later version of this book I will give more details to the state of said. For the time being every Swiss archaeologist should have a slight idea what is burial-ground consist of. You can download the data using the following link: muensingen_fib.csv. The data is such represents different fibulae found on the side. Please download the file and save it into a directory of your choice. For this chapter, like with all of the chapters, you might like to specify a specific folder for this chapter. Save the data there, and, as we have learnt in the last chapter, select this folder as your current working directory. If you have downloaded the data to this folder and correctly selected it as you’re working directory, you should be able to reach the Münsingen data and inspect its structure. Since it is a dataset in the “Continental” CSV file, you can load the data into are using to read.csv2() format. Also you will realise, that the dataset already contains row numbers. You might like to specify ‘row.names = 1’. muensingen &lt;- read.csv2(&quot;muensingen_fib.csv&quot;, row.names = 1) head(muensingen) # For getting a glimpse to the data ## Grave Mno FL BH BFA FA CD BRA ED FEL C BW BT FEW Coils Length fibula_scheme ## 1 121 348 28 17 1 10 10 2 8 6 20 2.5 2.6 2.2 4 53 B ## 2 130 545 29 15 3 8 6 3 6 10 17 11.7 3.9 6.4 6 47 B ## 3 130 549 22 15 3 8 7 3 13 1 17 5.0 4.6 2.5 10 47 B ## 8 157 85 23 13 3 8 6 2 10 7 15 5.2 2.7 5.4 12 41 B ## 11 181 212 94 15 7 10 12 5 11 31 50 4.3 4.3 NA 6 128 C ## 12 193 611 68 18 7 9 9 7 3 50 18 9.3 6.5 NA 4 110 C The dataset originally comes from the R package archdata. It is a data frame consisting of 30 observations with some variables describing the characteristics of the fibulae. If you want to full description of what the state of me, I suggest that you consult the documentation of the ice data package. There this dataset is called Fibulae. While we will graphically display different variables, I will explain what is variables mean. 3.2 Cross tables (contingency tables) The first category of visual representation of data is actually not a diagram. It is the table. Tables are today very widespread for the representation of information, so I don’t think that I will need to explain to you what the table looks and our table works. A bit formalised, how we use tables here, is that in most of the cases the rules will hold the items of investigation, while the columns will contain different variables. But this is not true all of the time. For a very specific type of table, that we will learn to know now, this is not true. The kind of table, that I’m talking about, is the contingency table. This kind of table is also known as cross tabulation, or crosstab. This kind of representation is used to show the interrelation between two variables. That means, contrary to what I have stated in the paragraph above, did hear both the rows and columns represent variables. Let’s have a look to one of these cross tabs, so it becomes clear, what is meant by that. For this, we will tabulate the scheme of the fibula against the grave in which it was found. Fibula Scheme is a standardised way of how different types of fibulae are are produced, they represent archaeological types. In one grave there may be more than one fibula. Therefore, the great number does not represent a unique identifier of the object fibula, the item of the investigation. It is just one variable among others. In R, you can use to come on table to display the number of fibula in a specific fibula scheme per grave. my_table &lt;- table(muensingen$fibula_scheme, muensingen$Grave) my_table ## ## 6 23 31 44 48 49 61 68 80 91 121 130 157 181 193 ## A 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 ## B 0 0 0 0 1 1 2 1 1 1 1 2 1 0 0 ## C 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 As you can see, the variable muensingen$fibula_scheme is given us the first parameter of the function, while the other variable muensingen$Grave represents the second parameter. The result is the table, in which the first parameter is mapped in the rows, while the second parameter, the grave number, is mapped to the columns. Each cell now represents the number of items, in this case of specific fibula types, in each burial. More abstract speaking, crosstab is the representation in which the current occurrence of two variable values are mapped. If we also want to have an idea, how many items there are per row and how many per column, we might like to add the margins to the table. Table margins gives us to sum of the values for each role, each column, and in total. An R, the command for that is addmargins(). addmargins(my_table) ## ## 6 23 31 44 48 49 61 68 80 91 121 130 157 181 193 Sum ## A 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 4 ## B 0 0 0 0 1 1 2 1 1 1 1 2 1 0 0 11 ## C 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 ## Sum 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 17 You can see, that we have a new row and a new column. The row contains the sum per column, while the column contains the sum per row. In the lower right most sell we have to total sum of all items. In data sets of low dimensionality, this represents a rather straightforward and convenient way to investigate the relationship between two, probably more, variables. They also represent a starting point for different other statistical approaches and techniques, for example the Chi-square test, that we will learn about later. In the context of spreadsheet software, crosstabs are often also called pivot table. 3.3 Basics about charts Besides the tables, which also contains some graphical elements, like lines, the visualisation, which comes to the minds of most people in respect of statistics, are charts. Most charts or diagrams contain a certain set of elements, that can repeatedly be seen even with different types of graphical display. Most of the time we have some axis, which represents the structure of the variable underlying to representation. Quite often, this access has some marks, most of the time regularly spaced, and often also with some annotation. These marks are called tick marks. For the representation of one variable, one axis might be enough. But most of the time, we have a two-dimensional visualisation. This is already necessary, if we want to represent the category of some items and the count of items in that specific category. Very often, we have charts that represents the relationship of two variables. In both cases, we have two axis. In the area, that is defined by the axis, we have to representation of the actual data. This takes place, using different symbols, lines, or other graphical elements. Here, different types of charts different. Very often, we have also label for the axis, labels for the whole plot, and sometimes some subheadings describing more in detail, what the plot is about. There are certain rules or guidelines, how shots should be designed, to be most efficient. Edward Tufte, a professor emeritus of statistics and computer science at University, is well known for his publications in respect to data visualisation. In one of his publications, he defined the principles for a good graphical representation of information. Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space. E. Tufte 1983 Just rule of thumb it also known as the data-ink-ratio, the ‘proportion of a graphic’s ink devoted to the non-redundant display of data-information’. As these are only guidelines, it is clear, that whole details are charged will be very much depends on its use case. But it’s also clear, that one should aim for a reduced use of graphical elements, so that the information, that needs to be transmitted, is in the foreground. Also, certain bells and whistles might enhance this information transmission quite a bit. I trust your sense of style to choose the right amount of ink for the right purpose. 3.4 The command plot() The command plot() is the basic commands in R to produce a graphical visualisation. What happens, when you do use plot with different data sets very much depends on, what kind of dataset you have. This command is a kind of chameleon, changing its appearance according to the necessities. This philosophy is also true for different other commands in R. There are some standardisations, that comparable effects should result from comparable names. So, whenever you use the command plot(), a plot will be the result. How this plot looks like, depends on the package, from which the data structure is coming, that you are plotting. The same is true for example for the command summary(). This command gives a summary, least surprisingly, good structured according to the data underline the summary. 3.4.1 Basic plotting Lets come back to our command plot(). It’s a basic manifestation can be seen, when we plot one variable. For this purpose, let’s take the length of the fibulae from our dataset. plot(muensingen$Length) You can see, that like with most other commands in R, within the brackets the parameters are written. Here it is a reference to the variable, that should be plotted. You also should be able to see elements from the general outline of the plot, that we just have introduced. In this basic implementation, the values to be plotted are visualised using the Y axis, while the X axis represents the order of the values in the vector. The values in the dataset themselves are represented as points, positions according to their actual value on the Y axis, and to their order in the dataset on the X axis. That is why the label of the X axis is index. Some standard layouts in respect to specific data can be selected using the parameter ‘type’. For example, type=\"p\" is the default setting and results in the plots that we just saw. Specifying type=\"b\" results in a plot of the points connected by lines, as you can see below. plot(muensingen$Length, type = &quot;b&quot;) But this kind of visualisation is not correct here. Line implies that there is a continuous process going on, which is not the case between the individual values of our unconnected similar. Better representation of the nature of our data might be, if we are using the parameter type=\"h\", That gives us vertical lines from the origin of our coordinate system to the actual value. plot(muensingen$Length, type = &quot;h&quot;) Below you can find a list of possible options: p – points (default) l – solid line b – line with points for the values c – line with gaps for the values o – solid line with points for the values h – vertical lines up to the values s – stepped line from value to value n – empty coordinate system The option \"n\", although seemingly quite useless, will become one of the most interesting options to be selected here. This option can be used, to draw a coordinate system, without filling it in the first place. In this way, we can ask R to draw to coordinate system, that we can then fill with our own symbols or other graphical elements. As I said, the command plot() is a chameleon that changes its appearance according to the dataset. For example, if we used to command on a dataset containing factor variables, the resulting visualisation will be a bar chart, counting the number of items per category, instead of the kind of visualisation that we have seen with the length data. plot(as.factor(muensingen$fibula_scheme)) 3.4.2 Enhancing the plot with optional components &amp; Text Of course, we can influence all the different elements of shops in such a flexible software like R. For example, we can specify the size of our X and Y axis, we can change the labels of both axis and also the heading and the subtitle of the chart. This is done using different parameters. By adding up so many parameters, the commands can become quite intimidating. But it is essentially just adding up parameter by parameter. So from a structural point of view there is no complex logic behind that. Also, when writing R commands, You can always us new lines behind elements that indicate, that our command is not finished yet. Such elements might be a mathematical symbols or for example commas like you can see below. .tiny[ plot(muensingen$Length, muensingen$FL, xlim=c(0, 140), # limits of the x axis ylim = c(0, 100), # limits of the y axis xlab = &quot;Fibula Length&quot;, # label of the y axis ylab = &quot;Foot Length&quot;, # label of the x axis main = &quot;Fibula total length vs. Foot Length&quot;, # main title sub=&quot;example plot&quot; # subtitle ) ] So you can see the effects of the different parameters. The extent of the axis is defined by xlim and ylim. The labels of the axis is given by xlab and ylab. The explanatory headings are defined by main and sub. Also, this plot represented by various plots, in which we met do you already know length of the fibula against the length of the foot of the fibula. It is quite obvious, that the longer the fibula is, the longer also its foot by be. So using this kind of so-called scatterplot, we can visualise this relationship between the two variables. Plot is doing a lot for you: Opens a window for display Determines the optimal size of the frame of reference Draws the coordinate system Draws the values In the background, also the last plot is remembered, and this plot is still changeable. You can use specific commands, to add elements to the already existing plot. These elements can be: lines – additional lines to an existing plot points – additional points to an existing plot abline – additional special lines to an existing plot text – additional text on choosen position to an existing plot This is the reason, why it sometimes might be reasonable, to plot an empty plot using the option “n”. You can use this version, to add first create a coordinate system, and then fill it up with lines or points. There are some more possibilities to change the layout and style of the plots. For example, the command par() will give you the tools to change a lot of the look of and feel. I suggest that you look up the help page for this command to see which possibilities exists to change the layout of the plot. ? par As an example, how you can add elements to an existing plot, we will draw some straight lines into the plot of the total length versus the foot length of the fibula. At first, we will draw a line in red at the mean value of the length of the fibula. Since the length is on the X axis, the line with the mean of the length must be a vertical line going up. Accordingly, the mean of the foot length, that is represented by the Y axis, must be a horizontal line. For both, we are using to command abline(). The difference is, that for vertical line specify the parameter ‘v’, for a horizontal line we specified a perimeter ‘h’. In both cases the parameter ‘col’ specifies the colour of the line. The third line is somehow special: it represents the relationship between the foot length and the total length in the data. Therefore, it is a diagonal line. It is defined by linear model of these two parameters. What this means we will explain later, for now you can just keep in mind, that in this way we can represent the trends in the data, drawing a trend line. abline(v = mean(muensingen$Length), col = &quot;red&quot;) # draw a red vertical line abline(h = mean(muensingen$FL), col = &quot;green&quot;) # draw a green vertical line abline(lm(FL~Length, data = muensingen), col = &quot;blue&quot;) # draw a blue diagonal line 3.5 Export the graphics Of course, if we have created a decent plot, we probably don’t want it to remain in R. Most of the time, we would probably like to use it in other contexts, be at homework, an article or a presentation. To do that, we need to wait to explore our plots. Using RStudio, probably the easiest way is to be used to graphical user interface, especially when it comes to exporting only individual plots. For this, in the plots window, there is a button called export. Here, you can select if you want to export your plot as an image (roster image), or as a PDF (vector file). The export windows should be more or less self-explanatory, you can specify the size of the resulting image and also the location, where it should be saved. Before RStudio, saving plots took place most of the time using commands from the command line. This still is very useful, if you use scripts and want to create multiple or even very many plots at once, changing the inputs data. Two very common formats when it comes to vector format or ‘PDF’ or probably more common ‘eps’ specifically for images. You can copy your current plot window to a vector file using the command dev.copy2...() and then the file format in which the result should be saved. So let’s save our current plots to both formats using to come on spill. dev.copy2eps(file=&quot;test.eps&quot;) dev.copy2pdf(file=&quot;test.pdf&quot;) There are even more raster then vector file formats. By default, R is capable of exporting to PNG, JPEG, TIFF and BMP. You can use the command savePlot() for this. savePlot(filename=&quot;test.tif&quot;, type=&quot;tiff&quot;) If you really plan to use the graphical visualisation in R in that way, it is worthwhile to dive deeper into the export format and options then we can present here. For most of the basic use cases, exporting the files via the graphical user interface is the most convenient and most controllable way of storing your valuable plots on your file system. 3.6 Pie chart Let’s not start discussing different plot types. We will begin with one of the most widespread used type of plots: the pie chart. You can see pie charts all over the media, newspapers, and also books, might it be scientific or popular books. Pie charts are used to display proportions of the total. For this reason, they are most suitable (if at all) for nominal data. Or, of course, percentage data, if this is the original data type. For example, results of the election that often represented in pie chart. Here, we see the percentage of voters voting for a specific party. Basically, this represents normal data, the choices of the individuals for one or the other party. I would like to opportunity to throw in an unnecessary formula here: \\[ a_i = \\frac{n_i} {N} * 360° \\] The proportions of the categories \\(i\\) in relation to the total number, represented by the number of items of that category \\(n_i\\) divided by the total number \\(N\\) is multiplied by 360°. The resulting angle is the angle, which can be used for a circular visualisation of this amount. Pie charts have different disadvantages, some they share with other graphical representation of data, but some are unique to this kind of display. Examples to colour selection very influential when it comes to perception. Red as an aggressive colour is perceived larger then for example grey. Very specific to a pie chart is the fact that we as humans are more trained to see differences in length then in area. And in the pie chart, the differences are visualised using the area of the different pieces of the pie. This results in effect, that small differences are not so easily visible in a pie chart, then this would be the case for example with a bar chart, a visualisation technique that we will learn about below. Like in many other cases, 3-D representation does not work so well on printed paper. Since the human eye and brain must compromise between both the perception that we are looking at a 3-D image but in reality it’s a 2-D object, differences are distorted. Let’s look at the following example that I took from the literature: .caption[source: http://www.lrz-muenchen.de/~wlm] The pieces »viel zu wenig«, »etwas zu wenig« und »gerade richtig« have exactly the same size, the piece »viel zu viel« is a bit smaller. Perception wise, the different shares seem to be quite different because of the reasons mentioned above. So in any case, 3-D or not, pie charts are inferior to a lot of other visualisation techniques. Nevertheless, because it is a very widespread used technique, still I would like to demonstrate how are you can create them using R. The actual commands to draw a pie chart in R is pie(). This command expects the number of counts, and all the normalisation to percentages and then the visualisation will take place automatically. This means, that it might be necessary to recode data. In this case, we will use the fibula schemes and visualise there ratio. This variable comes in the form of a character vector indicating to different schemes. Here, we can use the command table, to transform the nominal presence into a table of counts. table(muensingen$fibula_scheme) ## ## A B C ## 4 11 2 pie(table(muensingen$fibula_scheme)) Do you original colour scheme of the pie() command is rather pastel. Of course, you can change the colours, using the col parameter. Here, in the order of their appearance, you can specify different colours, that will be used to represent this category. pie(table(muensingen$fibula_scheme), col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;)) This should be enough to be set for the pie chart. It had already had the honour to be the first mentioned. That’s enough. We will from now on turn to more scientifically useful visualisations. 3.7 Bar plot A worthy replacement for any kind of pie chart is a bar chart or a plot. Most of the time it is the better alternative. Since here, the differences are represented by the length of the bars, humans can more easily perceive the differences between different categories. Also, this kind of visualisation is more flexible, because you can also represent absolute data and measurements with a bar chart, not only percentages. Also, the command barplot() requires a vector containing the data. This can be the number of fibulae in the different styles, like so: barplot(table(muensingen$fibula_scheme)) Or it can be length of the different fibulae. barplot(muensingen$Length) Both versions are meaningful and can help visualising the data. Although especially the last plot is difficult to understand, because it it lacking some essential information. For example, what is represented here by the bars, in total, but also, what does the individual bars represent. Since the vector resulting from the table commands automatically had names, these names were used in the case of the fibula scheme. In the case of the length, we have to specify these names on our own. For this, there is the parameter “names.arg”. Also, you might like to turn the names so that they become more readable and do not flow into each other. For this, you can use the parameter “las=2”. Lastly, I’ll give you a variant of how you can put the main title to a plot using to command title(). par(las=2) # turn labels 90° barplot(muensingen$Length, # plot fibulae length names.arg=muensingen$Grave) # with names of the graves title(&quot;Fibulae length&quot;) # add title Of course, you can also turn the bar chart around, making it horizontal. In that case, you probably would like to turn the labels again. Also, you can influence the size of text using a parameter ‘cex’. You can also specify what should be changed in size, in this case the names. par(las=1) # turn labels back again barplot(table(muensingen$fibula_scheme), # Plot counts fibulae scheme horiz=T, # horizontal cex.names=2) # make the labels bigger \bBar charts are also much more flexible compared to pie charts in so far, as you can easily display more than two variables. In this way, your plot can become 3-D or more in a meaningful way. Let’s assume, we want to visualise the number of coils of the fibula in relationship to the style. We can use the table command to produce a table accordingly. my_new_table &lt;- table(muensingen$fibula_scheme, muensingen$Coils) my_new_table ## ## 3 4 6 10 12 ## A 1 3 0 0 0 ## B 0 3 6 1 1 ## C 0 1 1 0 0 No, we can directly put this table into the barplot() command. Let’s see the output: barplot(my_new_table) You can see, that the different styles (fibula schemes) get different colours. With this, we not only seeing the number of items in respect to the different number of coils, but also at the same time, in which style to fibula is produced. This way of representing subcategory it’s called ‘stacked’. If you don’t like this way of representation probably you like more the version where the different categories are put side-by-side like below. barplot(my_new_table, beside=T, legend.text=T) Until now, we have only seen bars of different height. The beauty of the pie command was, that it automatically transformed absolute values into percentages. With a slight alteration of the table commands, we can achieve the same with a bar chart, and even better. For that, we are using the prop.table() command. This stands for proportional table. In current versions of R, you can use also the command proportions(). Its first parameter is the dataset for which the proportion should be calculated, in the format of the table. That means, it takes the result of the table command, and then transform it into a proportional table. The second parameter defines, what should sum up to 100, or in other words, what is the full total to which the proportion should be calculated. As always in R rows come first, so they have the number 1, while columns come second, so they have the number 2. That means, the following command will calculate the percentages in respect to the columns, which each will sum up to 1. table.prop&lt;-prop.table(my_new_table,2) table.prop ## ## 3 4 6 10 12 ## A 1.0000000 0.4285714 0.0000000 0.0000000 0.0000000 ## B 0.0000000 0.4285714 0.8571429 1.0000000 1.0000000 ## C 0.0000000 0.1428571 0.1428571 0.0000000 0.0000000 If we put the results into the barplot() command, we will get a result comparable or even better to the pie chart. barplot(table.prop) Of course, you can also change elements of the bar plot. For example, you can get fancy with the colours. Here, are used to command rainbow, specified with the number of colours I would like to get, to create a colour spectrum from the rainbow. Also, I would like to have a legend. Additionally, I want to have a title. This title is quite long, so I divided it in two rows, using the special “” sign. Since it is such a long title, I also have to use space outside of the actual plot area. You can get a feeling for the effects of this different para metres by playing around a bit with them. barplot(table.prop, legend.text=T, # add a legend col=rainbow(3) # make it more colorful ) # add a title title(&quot;ratio of fibulae schemes \\n by number of coils&quot;, outer=TRUE, # outside the plot area line=- 3) # on line -3 above But also bar plots do not solve every problem that we have with graphical representation. For example, there is often the question, what is better: percentage or absolute numbers. If we would like to compare different situations, with different total numbers, and we are most interested in the ratios, then the percentages are a good choice. But at the same time, due to this characteristic, they can hide differences in the underlying total numbers. This can become especially problematic, if you divide your bars also in subcategories. par(mfrow=c(2,1)) barplot(my_new_table,beside=T) barplot(table.prop,beside=T) Just from the visualisation of the percentages here it seems, as if there are more fibulae of scheme A with three coils then with four. So it is absolutely necessary to always provide the absolute numbers, if you present your data as percentage of the total. It can take place in the caption, or directly in the plot. And this problem or better this consideration must be taken into account not only with bar charts, but also with any other representation of percentages. Another source of visual confusion can be the scales already ranges of the axis. For example, if we do not draw an access from 0 to the maximum value, but let it start at an arbitrary value, small differences can visually become very big. In the example below, I visualise the first and the second fibula respectively their length. par(mfrow=c(1,2)) barplot(muensingen$Length[1:2],xpd=F,ylim=c(45,55)) barplot(muensingen$Length[1:2],xpd=F) par(mfrow=c(1,1)) Although it is obvious, if you actually look at the axis, there is a difference it’s not that big (it’s only six), only from visual inspection it seem to be enormous compared to the visual representation of the same difference in the diagram to the right. Most of the time, it is better to have your axis ranging from 0 to the actual values, except for situations, where the comparison between different bars or other elements is hindered by the fact that the relative differences are so little. You might have realised, that in the last two examples are used to command par(mfrow=c(..)). With this comment, you can find that plots are placed side-by-side or one on top of the other. Also here do usual rule of R present: Rows are the first number, columns are the second number. So both I said that I want to have two rows of plots with one column, below I said I want to have one row with two columns. You can use this to lay out your plots in a smarter way. 3.8 Box-plot (Box-and-Whiskers-Plot) The next type of plot is not totally dissimilar to the bar plot. Also in this case, in the case of the box plot, we have a rectangular element representing our data. But the logic behind the box plot is very different. A box plot, which is also called box and whisker plot, is used to describe the distribution of values in a range of data. Let’s try to explain that using the numbers from 1 to 9 as our values. If we sort these values, than we get the root of numbers from 1 to 9. 1 2 3 4 5 6 7 8 9 ____|___|___|____ No, we divide the values by their position. That value, that is placed in the centre, is of specific importance because it is the most centred value of this dataset. Here we draw a line. If we take half the number between the first and the centremost, we get the first quarter of our data. When we do the same towards the end, we get the last quarter of the data. Where the first quarter ends, and we are the last quarter starts, we also mark this dataset. The values at these positions are 3 for the start of the first quarter, 5 for the most central value, and 7 for the start of the second quarter. Compare this with the following distribution of data. 1 1 2 7 7 8 20 26 100 ____|___|___|________ Here, two marks the beginning of the second quarter, seven customers sent a value, and 20 marks the beginning of the last quarter of the data. Notice, that this does not depend on their actual values, but only on the position within the ordered dataset. If we now have a Y axis, on which we have continuous scale of the actual values, and we draw a box according to the parameters we just defined (position of the first the second and the third quarter of the data), than we get a feeling for how the values of the data within our dataset are distributed. In the visualisation of the box plot, the box marks to inner half of the data so the second and the third quarter of the data. The border between the second of the fourth quarter, the most central value, is marked by a line (This value is also “median”, we will learn about it soon). Beside the box itself at the thick line dividing it, there are also thin lines sticking out from the box on top and bottom. This lines are called whiskers. The end line of a whisker is drawn at that value, that is less than 1.5 times the distance of the inner half of the data away. Every other point, that is more far away, is considered to be an outlier, and is visualised by a point. Let’s see the actual box plot of all numbers 1 to 9. boxplot(1:9) Here you can see all elements I’ve just described except for the outliers. To produce an outlier, we will add a very high value to our dataset. boxplot(c(1:9,15, 20)) You can see, did I edit the value of 15, which is now marked by the whisker. I also edit the value 20, which is now displayed as an outlier. If we applied to our actual data, the length of the fibula at Münsingen, you will see kind of the same. boxplot(muensingen$Length) The interpretation would be like following, most of the data are evenly distributed between 30 and 80 mm. The thick line is a bit lower in the box, this means, that there are slightly more low values than high values. Besides the usual values, we have two outliers at approximately 120 mm plus or minus. By inspecting this plot, I already learnt a lot about distribution of the data in our dataset. Box plots are especially useful, if you want to compare the distribution of data between different categories of data. For example, you might like to compare the distribution of the length of the fibula in respect to their style. For this, I will introduce to you in new syntax in how are you can formulate circumstances in R. This notation is called formula notation. It is centred around the tilde ‘~’. This sign means “in relation to”. So, if I like to draw a box plot of the length of the fibula in respect to its style, I can express it like a below: boxplot(muensingen$Length ~ muensingen$fibula_scheme) We will work with this kind of formula notation more in more advanced topics here. This notation becomes especially helpful when it comes to modelling. Because there, you model things in relation to other things. Back to our box plot. Of course, we can also use parameters here, to add elements to our plot. This elements might be a title, the colour, or the labels of the axis. You might like to play around a bit with the example below to get a feeling of the effects. par(las=1) boxplot(Length ~ fibula_scheme, data = muensingen, main = &quot;Length by type&quot;, col=&quot;green&quot;, xlab=&quot;fibulae scheme&quot;, ylab= &quot;length&quot; ) All in all, a box plot is a very helpful tool when it comes to condensed have a look on the distribution of data. As I have explained, this is specifically helpful if you want to compare different distributions with each other. If you are more the girl that likes to watch the matrix uncoded, the scatterplot is probably more your type. We will learn about that below. 3.9 Scatterplot Just get a proper is probably that kind of plot of people (beside a pie chart) imagine most of the time when they think about statistical visualisation. It is also one of the most basic plot types. That’s why it is also the standard configuration of the command plot(). Basically it is used to display one variable in relation to another one. This other variable can also be the order of the values, as we have seen in the example above. In general, scatterplot is suited for all data types and scales of variables, although most of the time for nominal and ordinal data other chart types might be preferable. Since we discussed basic elements of the scatterplot already above, without mentioning the name, I would like to use the space here to show you some alternative ways how are you can produce a scatterplot in R. For this, we start with the basic plot of the similar length against the foot length, with the Trent line edit in red, like we have done before. Although libraries offer a different ways of displaying scatterplots. One option here is the library car, that is specifically used for regression analyses (analyses of the relationship of two variables). To get access to the functionality of another library in R, at first we have to load this library. For this we used to come on library(). As the parameter you use the name of the library that need to be loaded. Pay attention: you don’t need to use quotation mark \" to load known and installed libraries. The command from ‘car’ to produce a scatterplot is scatterplot(). In here you also use the formula notation that we have seen already with the box plot, but also with the linear model for the trendline in the example above. Here you specify the names of the columns in the dataset as variables, and as data you give the name of the variable which holds the whole dataset, the data frame muensingen. library(car) # library for regression analysis scatterplot(FL ~ Length, data = muensingen) At this resulting scatterplot, you can see different elements, that we have seen before: for example, the box plots. Also, we see a trend line. But also, we see more lines. If you want to know, what is lines mean, you should consult do you help for the function scatterplot(). Luckily, you know how this can be done. Another suggestion is to use the library ggplot2. This library is very powerful and it is used a lot in professional visualisation. In publications and in conference presentations you will see ggplot style visualisations very often. The reason, why we are not using it here, is, that it comes with its own syntax philosophy. To learn this, would overwhelm potentially students that already have to cope with the basic understanding of R. But once you have mastered R in general, I strongly suggest that you have a closer look to this plot library. library(ggplot2) # advanced plots library b&lt;- ggplot(muensingen,aes(x=Length,y=FL)) graph&lt;-b + geom_point() show(graph) 3.10 Histogramm The next type of plants that we would like to have a look to is the histogram. Here, we take a different perspective compare to the scatterplot, and more similar to the box plot. Also here, we are looking at the distribution of the data. We visualise, in which part of the value range of our data most of our data are located. So what we will see, is if most of the data rather low or rather high values, for example. At first I would like to give you an example of an histogram, so that we can understand its character and its use cases. Therefore let’s again use the length of the fibula in our dataset and plot the histogram accordingly. hist(muensingen$Length) You can see that on the X axis of the plot we can see the actual values. The Y axis has to label frequency. The data are represented like with a bar plot in between values of length. In the first bar, between 20 and 40, we have to representation of all fibulae with a length between 20 and 40. If we look to the frequency, we can see that there are three. The next class of fibulae is between 40 and 60. Here, we can see that we have 10. This goes on. So, in histogram, we don’t see any longer the individual values, but we see, how many items have values within a certain range. With this visualisation and the perspective on the distribution we are reducing the complexity of the data. You don’t any longer see the individual item, but we get a better understanding about the distribution of the values of the individual items within the whole dataset. Quite often in visualisation, but also statistics in general, we have to compromise between the consideration of the new visual case and the extraction of the general pattern. In the standard display of histogram, we can only guess the individual values or the total numbers of cases per class. If we add labels, we can also see the actual numbers represented in the plot. hist(muensingen$Length, labels = T) Of course, we are not forced to only use the classes between 20 and 40, 40 and 60 and so on. We can also define our own classes. For example, if we want to have a finer resolution, we could decide to display classes off with of 10. We do that, using the parameter ‘breaks’. hist(muensingen$Length, labels = T, breaks = 10) Please note the differences: before, we had some blocks, that now are divided into finer structures. The choice of the class with can be decisive for the interpretation. Although our dataset is rather small, if you look to the highest values, just from visual inspection in the first case it seems that we have a rather constant data distribution between 100 and 140. With the smaller class with, the holes in this distribution become obvious. So also here, we have to make a compromise between visualising the individual case and total pattern. Again, of course, we can use the usual suspects to change the look of our histogram. The disadvantages of the Instagram are, that makes the data reduction necessary and therefore we lose some information in the visualisation. Also, as we have seen, is the actual display show me dependent on the choice of the class with. There are different techniques to overcome especially this problem. The first, the stem and leaf chart, comes from an age, where computers we are not able to produce plots. For reasons of completeness, but also because it’s mentioned in the Stephen Shennan’s book, we included here. The other and currently much more popular version is the kernel density estimation or kernel smoothing. We will learn about it afterwards. 3.11 stem-and-leaf chart The stem and leave chart is a clever idea to at the same time represent the general pattern, but also the individual cases. On the one hand, it is also a kind of histogram. But at the same time, it shows the values of the individual cases. Nevertheless, it has become a bit out of fashion lately. Let’s demonstrate it with our usual example. The command is stem. stem(muensingen$Length) ## ## The decimal point is 2 digit(s) to the right of the | ## ## 0 | 34444 ## 0 | 5555566677 ## 1 | 13 Our dataset consists of several similar length below 100, and two above 100. The latter you can see as the last line of the result of the stem and leave plot. The line above represents all the fibulae between 50 and 100. The top line represents all those between zero and 50. You can see, that the individual cases are represented by figures. This figure indicates the next value. So for example, in the top row the first number is zero. This means, that the first number is below 100. It starts with a 3, so it’s round about 30. After that, we have four 4s. This means, we have four more fibula with length of around 40. By now, you should’ve understand the pattern. So as promised, the steam and leaf plot is as such a clever idea to represent data. It’s only drawback is, that it doesn’t look very visual, and reminds a lot on the age of computers without decent graphical displays. And has also announced, there is some more modern alternative to that that we will have a look to in the next part: the kernel smoothing. 3.12 kernel smoothing (kernel density estimation) The last plot version that I would like to introduce to you is the kernel density estimation, quite often abbreviated as KDE. Also this visualisation is very similar to the histogram. Let’s have a look at it and then discuss its features. This time, we actually have to specify two commands. The first, density() is doing the actual calculation. The second is the usual plot() command. The density command is encapsuled into the brackets of the plot command. In this way, the output of the density serves as an input for the plot. plot(density(muensingen$Length)) You can see, that’s the essential elements are quite comparable to the histogram. We have an X axis visualising the values within the dataset. And we have a Y axis, this time not with frequency, but with density. This concept of density is the most difficult part to understand here. Therefore, let’s postpone it for a second. Let’s first concentrate on this moving part. For this, let’s assume that we are not looking at the actual value, But to a more blurred representation of the value. Like, if you are looking with half closed eyes or in not fitting glasses to a point. It will be blurred. The most intensity will still be in the centre, but there will be a halo of lesser and lesser intensity the more far you will get from the centre. Or probably a better way to understand this is to think about the actual values as hole in a vessel filled with sand. At the hole, which is at the position of the actual value, the sand will fall down and will form a heap. This heat will be the highest in the centre so at the actual value, but it will form a small hill, starting from left and right of the actual value. If two values are nearby, the hills will merge and join in a bigger hill. This is how you can interpret the picture above. Around 40 to 50, there are the most holes in our sandbox, so here most of the sand will fall down and for the big heap. On the other hand, at 110 and 130, there are only two holes each, so they’re only small hills will form. With this kind of visualisation, we avoid a problem to artificially draw boundaries between classes. Still, we can see a representation of the total distribution of the values within our dataset. And this is more precise than the box plot, because we get more information about the internal structuring of the data. We can also combine the KDE with histogram. For this, we have to bring both to the same scale. The scale of the KDE is such, that the total area under the curve of the will sum up to 1. We can also be scaled histogram to be in the same scale. For this, we used to parameter ‘prob=T’. hist(muensingen$Length, prob=T) lines(density(muensingen$Length)) We later will learn more about the concept of area under the curve, for no visionalisation is in the foreground, so we will stick with that. 3.13 Guidelines 3.13.1 Stay honest! Some final suggestions for guidelines, that you might like to consider, when you’re plotting data. The first and probably most relevant is that you should stay honest in your data representation. It is easy to cheat with different techniques. Although for example everyone probably can understand the scales of your axis, nevertheless the presentation can produce very different perceptions. The choice of the way how are you display your data has a strong influence on the statement and how it will be received. Choice of display has a strong influence on the statement. Let’s use the example of the Swiss stock market index to see, how different scales can influence the visualisation. The upper left panel shows the development of the Swiss stock market within the last year. Usually you can detect the crash that took place in the course of the Corona epidemy. This is the kind of display that is quite often shown in respect to such developments. The upper limit represents the upmost value, the lower limit the lower most value. Of course, this is visible at the Y axis. In this visualisation, the development looks very dramatic. If we only change the Y axis starting from zero, this crash looks less dramatic immediately. If we additionally enlarge our investigation window (or at least the window of data that we are displaying), it becomes obvious that much stronger deteriorations took place in the past. The lower right panel shows Shows the development starting from 1990. This shows, how low this value was in the beginning. However you might like to interpret this developments in respect of today’s severity, it is clear, that the different scales put this development in very different frames of references. So, you could sum up the suggestions for graphical representation like so: Stay honest! Choice of display has a strong influence on the statement. Clear layout! Minimise Ratio of ink per shown information! Use the suitable chart for the data! Consider nominal-ordinal-interval-ratio scale For the last point, I have compiled a small table that can give you an advice which kind of visualisation you should choose in which kind of situation. What to display suitable not suitable Parts of a whole: few Pie chart, stacked bar plot Parts of a whole: few Stacked bar plot Multiple answers (ties) Horizontal bar plot Pie chart, stacked bar plot Comparison of different values of different variables Grouped bar plot Comparison of parts of a whole Stacked bar plot Comparison of developments Line chart Frequency distribution Histogram, kernel density plot Correlation of two variables scatterplot Sometimes, it is illustrative to look at bad examples. Such bad representation of data is also called chartjunk. I will link here to the respective chapter of the book of Edward Tufte to you, so if you would like to go deeper into the subject, can I have a look here. But with his keywords, entered into a search engine of your choice, you will be delighted with a lot of examples of how not to do it. Enjoy. "]]
